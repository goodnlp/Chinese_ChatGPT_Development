# Chinese_ChatGPT_Development
## 目录
* 引言
* 数据的准备
* 模型编写和训练
* 研发过程中的收获，总结和反思

## 引言
ChatGPT已经引发信息时代的又一轮革命，但是中文世界的模型发展却很滞后，本项目主要目的是在中文ChatGPT模型的研发做一些探索，这个项目也只是用于教学或者研究目的，由于计算资源和人力有限，并不能用于生产上线服务。<br />
机器学习的研发包含三个基本要素，数据，模型，算力，本项目着重于前两个因素。

## 数据的准备

(1) 预训练语料
* 维基百科中文词条的所有数据（2019版），文件大小518.7M, 链接: https://pan.baidu.com/s/1eCE_Ez_fTA4AllkCds-x_Q?pwd=3242 提取码: 3242
* 百科问答数据集（2019版本），文件大小650M,  链接: https://pan.baidu.com/s/1ZB-rX3bj0xtrp1t_yA2Xmw?pwd=2023 提取码: 2023

(2) 有监督训练数据集 <br />
有监督训练是指，给定一个特定的输入，模型应该学习到正确的输出。比如情感分析任务，输入“我今天很高兴”，那对应的输出应该是积极情感。这里其实就是训练instruction GPT的过程了。

* 分类任务数据集
    * 头条数据集，链接: https://pan.baidu.com/s/1jHnUpF_F94Z1f-IWFmWvDA?pwd=vdt4 提取码: vdt4 
    
(3) 多轮对话数据集
openai的chatGPT的一个明显能力是具有很强的感知上下文的能力，这就让交互更加流畅自然。这样的能力的获得肯定是要特定的数据来训练的，因此本项目也会囊括下面的数据集。
*豆瓣多轮对话数据集，链接: https://pan.baidu.com/s/19s3Q4JROnxqVeJX7gnxEwA?pwd=2u9t 提取码: 2u9t

(4) 训练Reward model的数据集 <br />
这是需要人工标注的，就是上述模型针对一个输入，输出一个句子，人工需要对其打分，分数从1到5分布，人工需要判断，越符合人的需要，打分越高。<br />
这个数据集要多大呢，目前ChatGPT没有公布具体数字，只能先自行摸索了。<br />
当然，数据集数量大小是一方面，输入句子的种类分布均匀且具有代表性也很重要。<br />
由于这个项目的目的并不是“调教”出一个可以直接用于生产环境的模型，这个项目的目的在于把训练ChatGPT的过程走一遍，积累一些定性的经验，因此，最终标注的数据量不会多大。

## 模型编写和训练
(1) 模型代码编写

(2) 训练过程
* 训练脚本的讲解


## 研发过程中的收获，总结和反思
* 多GPU训练是很重要的，多GPU的使用可以训练更大的模型；如果只支持多卡多机器训练那就更好了，可以快速训练模型

