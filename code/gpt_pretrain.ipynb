{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess wikipedia_zh corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3c68c",
   "metadata": {},
   "source": [
    "## load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b04e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class utils(object):\n",
    "  def __init__(self,path=None):\n",
    "    self.path=path\n",
    "    #self.tokenizer = BertTokenizer.from_pretrained('../Language_model/bert-base-uncased')\n",
    "\n",
    "  def preprocess_wiki(self, path):\n",
    "    import pandas as pd\n",
    "    f = pd.read_csv(self.path)\n",
    "    f = f.dropna()\n",
    "    f.iloc[:,1] = f.iloc[:,1].astype(\"string\")\n",
    "    res = list(f.iloc[:,1])\n",
    "\n",
    "    vocab=set()\n",
    "    length=[]\n",
    "    sentence_arr=[]\n",
    "    for i in range(len(res)):\n",
    "      text=res[i]\n",
    "      tokenized_text = list(text) # special for chinese\n",
    "      # remove sentence which is too short, too long\n",
    "      if 5<= len(tokenized_text)<=100:\n",
    "        sentence_arr.append(tokenized_text)\n",
    "        length.append(len(tokenized_text))\n",
    "        vocab.update(tokenized_text)\n",
    "\n",
    "    v2i={v: i for i, v in enumerate(sorted(vocab), start=1)}\n",
    "    v2i['<PAD>']=0\n",
    "    v2i[\"<SEP>\"] = len(v2i) # <GO> as start of sequence ,<SEP> as end of sequence\n",
    "    v2i[\"<GO>\"] = len(v2i) # the total number of tokens should include these special tokens: len(v2i)\n",
    "\n",
    "    i2v = {i: v for v, i in v2i.items()}  \n",
    "    return sentence_arr, v2i, i2v, max(length)\n",
    "\n",
    "  def token_to_idx(self,sentence_arr, v2i):\n",
    "    sentence_idx=[]\n",
    "    for i in range(len(sentence_arr)):\n",
    "      sentence_idx.append([v2i['<GO>']]+[v2i[item] for item in sentence_arr[i]]+[v2i['<SEP>']])\n",
    "    return sentence_idx\n",
    "\n",
    "  # add a pad_zero function to align the sentences of various length\n",
    "  def pad_zero(self, seqs, max_len):\n",
    "      PAD_ID = 0\n",
    "      padded = np.full((len(seqs), max_len), fill_value=PAD_ID, dtype=np.int32)\n",
    "      for i, seq in enumerate(seqs):\n",
    "          padded[i, :len(seq)] = seq\n",
    "      return padded\n",
    "\n",
    "  def get_idx_sentence(self):\n",
    "    sentence_arr, v2i, i2v, max_len= self.preprocess_wiki(self.path) #input is part of wiki data, for demo usage\n",
    "    sentence_idx = self.token_to_idx(sentence_arr, v2i)\n",
    "    # define idx for padding\n",
    "    PAD_ID= v2i['<PAD>']\n",
    "    # there is <GO> and <SEP> at start and ending of sentence, so the full length should be 100+2=102\n",
    "    sentence_idx_padded = self.pad_zero(sentence_idx,max_len+2)\n",
    "\n",
    "    return sentence_idx_padded.tolist(), v2i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764595a",
   "metadata": {},
   "source": [
    "# define the module and gpt class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53340812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_k)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "        #self.embedding=nn.Embedding() ##\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor) -> Tensor:  ## 传入mask \n",
    "        query= self.q(query)\n",
    "        key= self.k(key)\n",
    "        value=self.v(value)\n",
    "\n",
    "        temp = query.bmm(key.transpose(1, 2))\n",
    "        scale = query.size(-1) ** 0.5\n",
    "\n",
    "        score=temp/scale\n",
    "        score=score+mask\n",
    "\n",
    "        softmax = f.softmax(score, dim=-1)\n",
    "        return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask) -> Tensor: ## 传入mask\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value,mask) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090608dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(\n",
    "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / 1e4 ** (dim // dim_model)\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): ##不是在这里传入mask\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tensors0: Tensor, tensors1: Tensor, tensors2: Tensor, mask: Tensor) -> Tensor:\n",
    "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        # self.mask=mask\n",
    "        return self.norm(tensors0 + self.dropout(self.sublayer(tensors0, tensors1, tensors2,mask))) ## 传入mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feed fowward network,Residual需要传入mask，这里不用，所以要分别开\n",
    "class Residual_ffn(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1): ##不传入mask\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"value\" tensor is given last, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        #self.mask=mask ##\n",
    "        return self.norm(tensors + self.dropout(self.sublayer(tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf080c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_k = dim_v = dim_model // num_heads\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v), ## 传入mask\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout, \n",
    "        )\n",
    "        self.feed_forward = Residual_ffn(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor,mask: Tensor) -> Tensor: ##传入mask\n",
    "        src = self.attention(src, src, src,mask) ###传入mask\n",
    "        #return src \n",
    "        return self.feed_forward(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1,\n",
    "        device: str = 'cpu' \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.device= device\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor) -> Tensor: ##可以传入mask\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        pos=position_encoding(seq_len, dimension) #\n",
    "        pos=pos.to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')) # load data to gpu\n",
    "        src += pos\n",
    "        for layer in self.layers:\n",
    "            src = layer(src,mask)  ##传入mask\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57efba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a trial of gpt model testing\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 4,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 8, \n",
    "        dim_feedforward: int = 2048//2, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        n_vocab: int=4,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_vocab, dim_model)\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            device= device\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(dim_model, n_vocab)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor) -> Tensor: ##传入mask\n",
    "        emb=self.embedding(src)\n",
    "        enc=self.encoder(emb,mask) ##传入mask\n",
    "        out=self.out(enc)\n",
    "        \n",
    "        return out ##no need softmax, nn.cross_entropy take care of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5acd7",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8dfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a square matrix where the each row allows one word more to be seen\n",
    "\n",
    "def generate_masks(src):\n",
    "    seq_len= src.size(1)\n",
    "\n",
    "    pad_int= [int(seq_len-i) for i in src.count_nonzero(dim=1)]\n",
    "\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len) == 1) # Lower triangular matrix\n",
    "    mask = mask.float()\n",
    "    mask = mask.masked_fill(mask == 0, -1e9) # Convert zeros to -1e9\n",
    "    mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "\n",
    "    mask_arr=[]\n",
    "    for i in pad_int:\n",
    "      mask[:,-(i):]= -1e9\n",
    "      mask_arr.append(mask)\n",
    "\n",
    "    masks=torch.cat(tuple(mask_arr),dim=0)\n",
    "    masks=masks.reshape(src.size(0),seq_len,seq_len)\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bb83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "######\n",
    "def loss_masked(output, src, loss_fn):\n",
    "    nonpad_int=src.count_nonzero(dim=1)\n",
    "    # discard pad elements\n",
    "    res=[]\n",
    "    for k,item in enumerate(nonpad_int):\n",
    "        res.append(src[k][:int(item)])\n",
    "\n",
    "    loss_res=0\n",
    "    for i in range(src.size(0)):\n",
    "        loss_res+=loss_fn(output[i], src[i])\n",
    "\n",
    "    return loss_res/src.size(0)\n",
    "######\n",
    "\n",
    "def train(model, data, batch_size):\n",
    "    torch.manual_seed(0)\n",
    "    #model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    torch.cuda.set_device(0)\n",
    "    model.cuda(0)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss().cuda(0)  ## this is for classification\n",
    "    opt = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "\n",
    "\n",
    "    n=len(data)// batch_size\n",
    "\n",
    "    # prepare for next word prediction\n",
    "    t0=time.time()\n",
    "\n",
    "    for i in range(n):\n",
    "        t0= time.time()\n",
    "        src=data[batch_size*i:batch_size*(i+1)]\n",
    "        src=torch.tensor(src).long()\n",
    "\n",
    "        _seq=src[:,:-1]\n",
    "        seq_=src[:,1:]\n",
    "        masks=generate_masks(_seq)\n",
    "\n",
    "        \n",
    "        #put to gpu\n",
    "        _seq=_seq.cuda(non_blocking=True)\n",
    "        seq_=seq_.cuda(non_blocking=True)\n",
    "        masks=masks.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(_seq,masks)\n",
    "        #print(outputs[0].shape)\n",
    "        #print(seq_[0].shape)\n",
    "        loss=loss_masked(outputs,seq_, loss_fn) # next word prediction\n",
    "\n",
    "        # the part of padding loss should be removed before backprop\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print(\"loss is:\",loss.detach().item())\n",
    "        t1=time.time()\n",
    "        print(t1-t0)\n",
    "        \n",
    "        if i%10000==0: np.savetxt('./model_dir/gpt_loss_%d.csv'%(i), np.array([loss.detach().item()]))\n",
    "\n",
    "    # save model parameters after finish training model\n",
    "    torch.save(model, \"./model_dir/model.pkl\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def train_mp(model, data, batch_size):\n",
    "    torch.manual_seed(0)\n",
    "    #model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    torch.cuda.set_device(0)\n",
    "    model.cuda(0)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss().cuda(0)  ## this is for classification\n",
    "    opt = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "\n",
    "\n",
    "    n=len(data)// batch_size\n",
    "\n",
    "    # prepare for next word prediction\n",
    "    t0=time.time()\n",
    "\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    for i in range(n):\n",
    "        t0= time.time()\n",
    "        src=data[batch_size*i:batch_size*(i+1)]\n",
    "        src=torch.tensor(src).long()\n",
    "\n",
    "        _seq=src[:,:-1]\n",
    "        seq_=src[:,1:]\n",
    "        masks=generate_masks(_seq)\n",
    "\n",
    "        \n",
    "        #put to gpu\n",
    "        _seq=_seq.cuda(non_blocking=True)\n",
    "        seq_=seq_.cuda(non_blocking=True)\n",
    "        masks=masks.cuda(non_blocking=True)\n",
    "\n",
    "        ## include mixed precision\n",
    "        opt.zero_grad()\n",
    "        # Casts operations to mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Forward pass\n",
    "            outputs = model(_seq,masks)\n",
    "            #print(outputs[0].shape)\n",
    "            #print(seq_[0].shape)\n",
    "            loss=loss_masked(outputs,seq_, loss_fn) # next word prediction\n",
    "\n",
    "        # Scales the loss, and calls backward()\n",
    "        # to create scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "\n",
    "        # Unscales gradients and calls\n",
    "        # or skips optimizer.step()\n",
    "        scaler.step(opt)\n",
    "\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        print(\"loss is:\",loss.detach().item())\n",
    "        t1=time.time()\n",
    "        print(t1-t0)\n",
    "        \n",
    "        if i%10000==0: \n",
    "            np.savetxt('./gpt_loss_%d.csv'%(i), np.array([loss.detach().item()]))\n",
    "            # save model parameters after finish training model\n",
    "            torch.save(model, \"./model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9378a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the entry to start training the model, with data, with specified parameter\n",
    "if __name__ == '__main__':\n",
    "    # load cpu or gpu name\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model parameter\n",
    "    MODEL_DIM = 256\n",
    "    N_LAYER = 4*3\n",
    "    N_HEAD = 8\n",
    "    # training argument\n",
    "    batch_size = 2\n",
    "    # load data\n",
    "    ut = utils(path='./file00')\n",
    "    d, v2i=ut.get_idx_sentence()\n",
    "    n_vocab= len(v2i)\n",
    "    m= GPT(num_encoder_layers= N_LAYER,dim_model= MODEL_DIM, num_heads= N_HEAD, n_vocab=n_vocab, device=device)\n",
    "    # start training\n",
    "    train(m, d, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear gpu and cpu memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ba57d",
   "metadata": {},
   "source": [
    "# load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class must be defined somewhere\n",
    "import torch\n",
    "PATH = './model_dir/model.pkl'\n",
    "model = torch.load(PATH)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b0b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc399475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d63a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da9f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
